{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM for Non-Linear Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of non-linear data is:\n",
    "\n",
    "![SVM's for Non-Linear Data Sets](./img/non_linear_svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we cannot find a straight line to separate apples from lemons. So how can we solve this problem. We will use the Kernel Trick!\n",
    "\n",
    "The basic idea is that when a data set is inseparable in the current dimensions, add another dimension, maybe that way the data will be separable. \n",
    "\n",
    "The example above is in 2D and it is inseparable, but maybe in 3D there is a gap between the apples and the lemons, maybe there is a level difference, so apples are on level one and lemons are on level two. In this case we can easily draw a separating hyperplane (in 3D a hyperplane is a plane) between level 1 and 2.\n",
    "\n",
    "Let's assume that we add another dimension called X3. Another important transformation is that in the new dimension the points are organized using this formula x1² + x2².\n",
    "\n",
    "If we plot the plane defined by the x² + y² formula, we will get something like this:\n",
    "\n",
    "![3d_SVM](./img/3d_svm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to map the apples and lemons (which are just simple points) to this new space. \n",
    "\n",
    "What did we do? We just used a transformation in which we added levels based on distance. \n",
    "\n",
    "If you are in the origin, then the points will be on the lowest level. As we move away from the origin, it means that we are climbing the hill (moving from the center of the plane towards the margins) so the level of the points will be higher. \n",
    "\n",
    "Now if we consider that the origin is the lemon from the center, we will have something like this:\n",
    "\n",
    "![Transformed SVM](./img/transformed_svm.png)\n",
    "\n",
    "Now we can easily separate the two classes. These transformations are called kernels.\n",
    "Popular kernels are: Polynomial Kernel, Gaussian Kernel, Radial Basis Function (RBF), Laplace RBF Kernel, Sigmoid Kernel, Anove RBF Kernel, etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example would be:\n",
    "\n",
    "![](./img/1d_svm.png)\n",
    "\n",
    "After using the kernel and after all the transformations we will get:\n",
    "\n",
    "![](./img/transformed_1d_kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after the transformation, we can easily delimit the two classes using just a single line.\n",
    "\n",
    "In real life applications we won’t have a simple straight line, but we will have lots of curves and high dimensions. In some cases we won’t have two hyperplanes which separates the data with no points between them, so we need some trade-offs, tolerance for outliers. \n",
    "\n",
    "Fortunately the SVM algorithm has a so-called regularization parameter to configure the trade-off and to tolerate outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularisation\n",
    "\n",
    "The Regularization Parameter (in python it’s called C) tells the SVM optimization how much you want to avoid miss classifying each training example.\n",
    "\n",
    "If the C is higher, the optimization will choose smaller margin hyperplane, so training data miss classification rate will be lower.\n",
    "\n",
    "On the other hand, if the C is low, then the margin will be big, even if there will be miss classified training data examples. This is shown in the following two diagrams:\n",
    "\n",
    "![](./img/reg_svm.png)\n",
    "\n",
    "As you can see in the image, when the C is low, the margin is higher (so implicitly we don’t have so many curves, the line doesn’t strictly follows the data points) even if two apples were classified as lemons. When the C is high, the boundary is full of curves and all the training data was classified correctly. \n",
    "\n",
    "\n",
    "**Note:** even if all the training data was correctly classified, this doesn’t mean that increasing the C will always increase the precision (because of overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples of SVM kernels\n",
    "\n",
    "- Polynomial kernel\n",
    "It is popular in image processing.\n",
    "Equation is:\n",
    "\n",
    "![](./img/polynomial-kernel.png)\n",
    "\n",
    "where d is the degree of the polynomial.\n",
    "\n",
    "- Gaussian kernel\n",
    "It is a general-purpose kernel; used when there is no prior knowledge about the data. Equation is:\n",
    "\n",
    "![](./img/gaussian-kernel.png)\n",
    "\n",
    "- Sigmoid kernel\n",
    "We can use it as the proxy for neural networks. Equation is\n",
    "\n",
    "![](./img/sigmoid-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "- Load the wine-quality dataset. \n",
    "- Split the data on train,test split on 80-20 ratio.\n",
    "- Build a Linear Regression model and Support Vector Machine to predict the dependent column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
